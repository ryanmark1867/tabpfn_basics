{"cells":[{"cell_type":"markdown","metadata":{"id":"YIeBggbihZkZ"},"source":["# NYC Airbnb Price Prediction - TabPFN model training\n","\n","Use dataset published by Kaggle - https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data - to train a simple XGBoost model to predict prices for Airbnb properties.\n","\n","This notebook contains the code to train the model from the dataset prepared in the airbnb_data_preparation notebook. It adapts the deep learning training notebook.\n"]},{"cell_type":"markdown","metadata":{"id":"DlRfewg3hZkg"},"source":["# Links to key parts of the notebook <a name='linkanchor' />\n","<a href=#ingestdash>Ingest data</a>\n","\n","<a href=#buildpipe>Build pipeline</a>\n","\n","<a href=#modelfit>Define and fit model</a>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sEt7qZr7hZkh"},"source":["# Common imports and global variable definitions"]},{"cell_type":"code","source":["\n","''' check to see if the notebook is being run in Colab, and if so, set the current directory appropriately'''\n","if 'google.colab' in str(get_ipython()):\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  %cd /content/drive/MyDrive/machine_learning_tabular_book/code/tabpfn_basics/notebooks"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gH8To-DKhoMZ","executionInfo":{"status":"ok","timestamp":1666403094166,"user_tz":240,"elapsed":1133,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"efe824f6-7698-4e81-d778-fc4bc1a0de7b"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/machine_learning_tabular_book/code/tabpfn_basics/notebooks\n"]}]},{"cell_type":"code","source":["import time\n","start_time = time.time()"],"metadata":{"id":"sFs8nj9ynE4S","executionInfo":{"status":"ok","timestamp":1666403094170,"user_tz":240,"elapsed":20,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# install TabPFN\n","!pip install tabpfn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ypX14XAFBDp8","executionInfo":{"status":"ok","timestamp":1666403097205,"user_tz":240,"elapsed":3052,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"41692250-2943-44e2-b92e-833ca8c12e86"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tabpfn in /usr/local/lib/python3.7/dist-packages (0.1.3)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (1.21.6)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (4.64.1)\n","Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (1.0.2)\n","Requirement already satisfied: configspace>=0.4.21 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (0.6.0)\n","Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (1.12.1+cu113)\n","Requirement already satisfied: gpytorch>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (1.8.1)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (6.0)\n","Requirement already satisfied: openml>=0.12.2 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (0.12.2)\n","Requirement already satisfied: seaborn>=0.11.2 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (0.11.2)\n","Requirement already satisfied: hyperopt>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from tabpfn) (0.2.7)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from configspace>=0.4.21->tabpfn) (1.7.3)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from configspace>=0.4.21->tabpfn) (3.0.9)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from configspace>=0.4.21->tabpfn) (0.29.32)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from configspace>=0.4.21->tabpfn) (4.1.1)\n","Requirement already satisfied: py4j in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.5->tabpfn) (0.10.9.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.5->tabpfn) (1.15.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.5->tabpfn) (0.16.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.5->tabpfn) (2.6.3)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.5->tabpfn) (1.5.0)\n","Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from openml>=0.12.2->tabpfn) (1.3.5)\n","Requirement already satisfied: minio in /usr/local/lib/python3.7/dist-packages (from openml>=0.12.2->tabpfn) (7.1.12)\n","Requirement already satisfied: xmltodict in /usr/local/lib/python3.7/dist-packages (from openml>=0.12.2->tabpfn) (0.13.0)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from openml>=0.12.2->tabpfn) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openml>=0.12.2->tabpfn) (2.23.0)\n","Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from openml>=0.12.2->tabpfn) (2.5.0)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from openml>=0.12.2->tabpfn) (6.0.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->openml>=0.12.2->tabpfn) (2022.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->tabpfn) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.2->tabpfn) (1.2.0)\n","Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.11.2->tabpfn) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn>=0.11.2->tabpfn) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn>=0.11.2->tabpfn) (0.11.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from minio->openml>=0.12.2->tabpfn) (2022.9.24)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from minio->openml>=0.12.2->tabpfn) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openml>=0.12.2->tabpfn) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openml>=0.12.2->tabpfn) (3.0.4)\n"]}]},{"cell_type":"code","execution_count":40,"metadata":{"id":"qMOg5J2MhZki","executionInfo":{"status":"ok","timestamp":1666403097209,"user_tz":240,"elapsed":115,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["# common imports\n","import zipfile\n","import pandas as pd\n","import numpy as np\n","import time\n","import seaborn as sns\n","import xgboost as xgb\n","from xgboost import XGBClassifier\n","from xgboost import plot_importance\n","from matplotlib import pyplot\n","# import datetime, timedelta\n","import datetime\n","import pydotplus\n","from datetime import datetime, timedelta\n","from datetime import date\n","from dateutil import relativedelta\n","from io import StringIO\n","import pandas as pd\n","import pickle\n","from pickle import dump\n","from pickle import load\n","from sklearn.base import BaseEstimator\n","from sklearn.base import TransformerMixin\n","# DSX code to import uploaded documents\n","from io import StringIO\n","import requests\n","import json\n","from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import os\n","import yaml\n","import math\n","import sys\n","from subprocess import check_output\n","from IPython.display import display\n","#model libraries\n","from tensorflow.keras.metrics import Accuracy, Recall, Precision\n","from tensorflow.keras.layers import Input, Dropout, Dense, BatchNormalization, Activation, concatenate, GRU, Embedding, Flatten, BatchNormalization\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.layers import BatchNormalization\n","#from tf.keras.layers.normalization import BatchNormalization\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras import backend as K\n","# from tensorflow.keras.utils.vis_utils import plot_model\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import load_model\n","from tensorflow.python.keras.callbacks import TensorBoard\n","#import datetime\n","#from datetime import date\n","from sklearn import metrics\n","# import pipeline libraries\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import average_precision_score\n","from sklearn.pipeline import Pipeline\n","from sklearn.base import TransformerMixin\n","from sklearn.base import BaseEstimator\n","from custom_classes import encode_categorical\n","from custom_classes import prep_for_keras_input\n","from custom_classes import fill_empty\n","from custom_classes import encode_text\n"]},{"cell_type":"code","source":["# TabPFN imports\n","import sys\n","import numpy as np\n","from pathlib import Path\n","import pandas as pd\n","import torch\n","import openml\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","from pathlib import Path\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris, load_wine\n","from sklearn.model_selection import train_test_split\n","\n","from tabpfn.scripts.transformer_prediction_interface import TabPFNClassifier\n","from tabpfn.scripts.decision_boundary import DecisionBoundaryDisplay\n"],"metadata":{"id":"QFf0fMLDBR1G","executionInfo":{"status":"ok","timestamp":1666403097211,"user_tz":240,"elapsed":114,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","execution_count":42,"metadata":{"id":"8fO-BXyehZkm","executionInfo":{"status":"ok","timestamp":1666403097213,"user_tz":240,"elapsed":114,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["#import tensorflow as tf\n","#tf.__version__ "]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cPDRPSYRhZkp","executionInfo":{"status":"ok","timestamp":1666403097216,"user_tz":240,"elapsed":115,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"d53f75ee-7876-4bbe-d408-0d695afa98e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["current directory is: /content/drive/MyDrive/machine_learning_tabular_book/code/tabpfn_basics/notebooks\n","path_to_yaml /content/drive/MyDrive/machine_learning_tabular_book/code/tabpfn_basics/notebooks/model_training_config.yml\n"]}],"source":["# load config file\n","current_path = os.getcwd()\n","print(\"current directory is: \"+current_path)\n","\n","path_to_yaml = os.path.join(current_path, 'model_training_config.yml')\n","print(\"path_to_yaml \"+path_to_yaml)\n","try:\n","    with open (path_to_yaml, 'r') as c_file:\n","        config = yaml.safe_load(c_file)\n","except Exception as e:\n","    print('Error reading the config file')\n"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ukTsqXUAhZkr","executionInfo":{"status":"ok","timestamp":1666403097218,"user_tz":240,"elapsed":103,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"8403a0c5-b40f-4e2c-cd49-9f20e245f992"},"outputs":[{"output_type":"stream","name":"stdout","text":["date today 2022-10-22 01:44:56.689134\n"]}],"source":["# load parameters\n","\n","repeatable_run = config['test_parms']['repeatable_run']\n","# fix seeds to get identical results on mulitiple runs\n","if repeatable_run:\n","    from numpy.random import seed\n","    seed(4)\n","    tf.random.set_seed(7)\n","\n","\n","testproportion = config['test_parms']['testproportion'] # proportion of data reserved for test set\n","trainproportion = config['test_parms']['trainproportion'] # proportion of non-test data dedicated to training (vs. validation)\n","get_test_train_acc = config['test_parms']['get_test_train_acc']\n","verboseout = config['general']['verboseout']\n","includetext = config['general']['includetext'] # switch to determine whether text columns are included in the model\n","save_model_plot = config['general']['save_model_plot'] # switch to determine whether to generate plot with plot_model\n","tensorboard_callback = config['general']['tensorboard_callback'] # switch to determine if tensorboard callback defined\n","\n","presaved = config['general']['presaved']\n","savemodel = config['general']['savemodel']\n","picklemodel = config['general']['picklemodel']\n","hctextmax = config['general']['hctextmax']\n","maxwords = config['general']['maxwords']\n","textmax = config['general']['textmax']\n","\n","targetthresh = config['general']['targetthresh']\n","targetcontinuous = config['general']['targetcontinuous']\n","target_col = config['general']['target_col']\n","\n","#time of day thresholds\n","time_of_day = {'overnight':{'start':0,'end':5},'morning_rush':{'start':5,'end':10},\n","              'midday':{'start':10,'end':15},'aft_rush':{'start':15,'end':19},'evening':{'start':19,'end':24}}\n","\n","\n","\n","emptythresh = config['general']['emptythresh']\n","zero_weight = config['general']['zero_weight']\n","one_weight = config['general']['one_weight']\n","one_weight_offset = config['general']['one_weight_offset']\n","patience_threshold = config['general']['patience_threshold']\n","\n","\n","# modifier for saved model elements\n","modifier = config['general']['modifier']\n","\n","# control whether training controlled by early stop\n","early_stop = True\n","\n","# default hyperparameter values\n","learning_rate = config['hyperparameters']['learning_rate']\n","dropout_rate = config['hyperparameters']['dropout_rate']\n","l2_lambda = config['hyperparameters']['l2_lambda']\n","loss_func = config['hyperparameters']['loss_func']\n","output_activation = config['hyperparameters']['output_activation']\n","batch_size = config['hyperparameters']['batch_size']\n","epochs = config['hyperparameters']['epochs']\n","\n","# date values\n","date_today = datetime.now()\n","print(\"date today\",date_today)\n","\n","# pickled original dataset and post-preprocessing dataset\n","pickled_data_file = config['general']['pickled_data_file']\n","pickled_dataframe = config['general']['pickled_dataframe']\n","\n","# experiment parameter\n","\n","current_experiment = config['test_parms']['current_experiment']\n","\n","# load lists of column categories\n","collist = config['categorical']\n","textcols = config['text']\n","continuouscols = config['continuous']\n","excludefromcolist = config['excluded']"]},{"cell_type":"code","source":["print(\"testproportion is \",testproportion)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kxFv_mwKHPF-","executionInfo":{"status":"ok","timestamp":1666403097221,"user_tz":240,"elapsed":95,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"b13ddf7d-96dc-424b-afa4-1054070cf449"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["testproportion is  0.02\n"]}]},{"cell_type":"markdown","metadata":{"id":"LHOm49gThZku"},"source":["# Helper functions"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"gID1ffP6hZkw","executionInfo":{"status":"ok","timestamp":1666403097224,"user_tz":240,"elapsed":89,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["# time_of_day = {'overnight':{'start':0,'end':5},'morning_rush':{'start':5,'end':10},\n","#              'midday':{'start':10,'end':15},'aft_rush':{'start':15,'end':19},'evening':{'start':19,'end':23}}\n","\n","\n","def get_time(hour):\n","    for tod in time_of_day:\n","        if (hour >= time_of_day[tod]['start']) and (hour < time_of_day[tod]['end']):\n","            tod_out = tod\n","    return(tod_out)\n","\n","def weekend_time(day, tod):\n","    if (day=='Saturday') or (day=='Sunday'):\n","        return('w'+tod)\n","    else:\n","        return(tod)\n","\n","\n"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"f7O0zkMphZky","executionInfo":{"status":"ok","timestamp":1666403097226,"user_tz":240,"elapsed":89,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["# get the paths required\n","\n","def get_path():\n","    '''get the path for data files\n","\n","    Returns:\n","        path: path for data files\n","    '''\n","    rawpath = os.getcwd()\n","    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n","    path = os.path.abspath(os.path.join(rawpath, '..', 'data'))\n","    return(path)\n","\n","def get_pipeline_path():\n","    '''get the path for data files\n","    \n","    Returns:\n","        path: path for pipeline files\n","    '''\n","    rawpath = os.getcwd()\n","    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n","    path = os.path.abspath(os.path.join(rawpath, '..', 'pipelines'))\n","    return(path)\n","\n","def get_model_path():\n","    '''get the path for data files\n","    \n","    Returns:\n","        path: path for model files\n","    '''\n","    rawpath = os.getcwd()\n","    # data is in a directory called \"data\" that is a sibling to the directory containing the notebook\n","    path = os.path.abspath(os.path.join(rawpath, '..', 'models'))\n","    return(path)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"UHr7ftVJhZkz","executionInfo":{"status":"ok","timestamp":1666403097228,"user_tz":240,"elapsed":89,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["def set_experiment_parameters(experiment_number, count_no_delay, count_delay):\n","    ''' set the appropriate parameters for the experiment \n","    Args:\n","        experiment_number: filename containing config parameters\n","        count_no_delay: count of negative outcomes in the dataset\n","        count_delay: count of positive outcomes in the dataset\n","\n","    Returns:\n","        early_stop: whether the experiment includes an early stop callback\n","        one_weight: weight applied to positive outcomes\n","        epochs: number of epochs in the experiment\n","        es_monitor: performance measurement tracked in callbacks\n","        es_mod: direction of performance being tracked in callbacks\n","    \n","    '''\n","    print(\"setting parameters for experiment \", experiment_number)\n","    # default settings for early stopping:\n","    es_monitor = \"val_loss\"\n","    es_mode = \"min\"\n","    if experiment_number == 0:\n","        #\n","        early_stop = False\n","        #\n","        one_weight = 1.0\n","        #\n","        epochs = 1\n","    elif experiment_number == 9:\n","        #\n","        early_stop = True\n","        es_monitor=\"val_accuracy\"\n","        es_mode = \"max\"\n","        #\n","        one_weight = (count_no_delay/count_delay) + one_weight_offset\n","        #\n","        get_test_train_acc = False\n","        #\n","        epochs = 20    \n","    elif experiment_number == 1:\n","        #\n","        early_stop = False\n","        #\n","        one_weight = 1.0\n","        #\n","        epochs = 10\n","    elif experiment_number == 2:\n","        #\n","        early_stop = False\n","        #\n","        one_weight = 1.0\n","        #\n","        epochs = 50\n","    elif experiment_number == 3:\n","        #\n","        early_stop = False\n","        #\n","        one_weight = (count_no_delay/count_delay) + one_weight_offset\n","        #\n","        epochs = 50\n","    elif experiment_number == 4:\n","        #\n","        early_stop = True\n","        es_monitor = \"val_loss\"\n","        es_mode = \"min\"\n","        #\n","        one_weight = (count_no_delay/count_delay) + one_weight_offset\n","        #\n","        epochs = 50\n","    elif experiment_number == 5:\n","        #\n","        early_stop = True\n","        # if early stopping fails because the level of TensorFlow/Python, comment out the following\n","        # line and uncomment the subsequent if statement\n","        es_monitor=\"val_accuracy\"\n","        '''\n","        if sys.version_info >= (3,7):\n","            es_monitor=\"val_accuracy\"\n","        else:\n","            es_monitor = \"val_acc\"\n","        '''\n","        es_mode = \"max\"\n","        #\n","        one_weight = (count_no_delay/count_delay) + one_weight_offset\n","        #\n","        epochs = 50\n","    else:\n","        early_stop = True\n","    return(early_stop, one_weight, epochs,es_monitor,es_mode)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g-Qgt5J-hZk0"},"source":["# Ingest data and create refactored dataframe <a name='ingestdash' />\n","- Ingest data for route information and delay information\n","- Create refactored dataframe with one row per route / direction / timeslot combination\n","\n","\n","<a href=#linkanchor>Back to link list</a>"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"DgI5ZiUShZk1","executionInfo":{"status":"ok","timestamp":1666403097231,"user_tz":240,"elapsed":90,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["def ingest_data(path):\n","    '''load list of valid routes and directions into dataframe\n","    Args:\n","        path: path for data files\n","    \n","    Returns:\n","        merged_data: dataframe loaded from pickle file\n","    '''\n","    file_name = os.path.join(path,pickled_dataframe)\n","    merged_data = pd.read_pickle(file_name)\n","    merged_data.head()\n","    return(merged_data)"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"eUn9rPCdhZk1","executionInfo":{"status":"ok","timestamp":1666403097233,"user_tz":240,"elapsed":91,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["def prep_merged_data(merged_data,target_col):\n","    '''add derived columns to merged_data dataframe\n","    Args:\n","        merged_data: input dataframe\n","        target_col: column that is the target\n","    \n","    Returns:\n","        merged_data: dataframe with derived columns added\n","    '''\n","    if targetcontinuous:\n","        merged_data['target'] = merged_data[target_col]\n","    else:\n","        merged_data['target'] = np.where(merged_data[target_col] >= merged_data[target_col].mean(), 1, 0 )\n","    return(merged_data)"]},{"cell_type":"markdown","metadata":{"id":"w4-DJIY8hZk2"},"source":["# Master Prep Cell\n","Contains calls to functions to load data, prep input dataframes, and create refactored dataframe"]},{"cell_type":"code","execution_count":51,"metadata":{"scrolled":false,"colab":{"base_uri":"https://localhost:8080/"},"id":"P_unjf8hhZk3","executionInfo":{"status":"ok","timestamp":1666403097421,"user_tz":240,"elapsed":277,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"098e9334-62f6-419e-cd87-3fe83b6b9c17"},"outputs":[{"output_type":"stream","name":"stdout","text":["path is /content/drive/MyDrive/machine_learning_tabular_book/code/tabpfn_basics/data\n","shape of pre refactored dataset (48895, 18)\n","shape of refactored dataset (48895, 18)\n","count under mean  34016\n","count over mean  14879\n","setting parameters for experiment  0\n","early_stop is  False\n","one_weight is  1.0\n","epochs is  1\n","es_monitor is  val_loss\n","es_mode is  min\n"]}],"source":["# master calls\n","\n","path = get_path()\n","print(\"path is\",path)\n","# load route direction and delay data datframes\n","merged_data = ingest_data(path)\n","merged_data = prep_merged_data(merged_data,target_col)\n","print(\"shape of pre refactored dataset\", merged_data.shape)\n","#merged_data['year'].value_counts()\n","#merged_data.groupby(['Route','Direction']).size().reset_index().rename(columns={0:'count'}).tail(50)\n","# create refactored dataframe with one row for each route / direction / timeslot combination\n","print(\"shape of refactored dataset\", merged_data.shape)\n","count_no_delay = merged_data[merged_data['target']==0].shape[0]\n","count_delay = merged_data[merged_data['target']==1].shape[0]\n","print(\"count under mean \",count_no_delay)\n","print(\"count over mean \",count_delay)\n","# define parameters for the current experiment\n","experiment_number = current_experiment\n","early_stop, one_weight, epochs,es_monitor,es_mode = set_experiment_parameters(experiment_number, count_no_delay, count_delay)\n","print(\"early_stop is \",early_stop)\n","print(\"one_weight is \",one_weight)\n","print(\"epochs is \",epochs)\n","print(\"es_monitor is \",es_monitor)\n","print(\"es_mode is \",es_mode)"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6xpjyG_hhZk3","executionInfo":{"status":"ok","timestamp":1666403097422,"user_tz":240,"elapsed":19,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"61898895-faf7-4ed4-aa67-5fc09f40f965"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(48895, 18)"]},"metadata":{},"execution_count":52}],"source":["merged_data.shape"]},{"cell_type":"markdown","metadata":{"id":"CAz0ZvIghZk4"},"source":["# Define training, validation, and test subsets of the dataset"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"z1WjDwUdhZk4","executionInfo":{"status":"ok","timestamp":1666403097423,"user_tz":240,"elapsed":17,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["def get_train_validation_test(dataset):\n","    '''get training and test data set\n","    Args:\n","        dataset: input dataframe\n","    \n","    Returns:\n","        dtrain: training subset of dataset\n","        dvalid: validation subset of dataset\n","        dtest: test subset of dataset\n","    '''\n","    train, test = train_test_split(dataset, test_size = testproportion)\n","    dtrain, dvalid = train_test_split(train, random_state=123, train_size=trainproportion)\n","    print(\"Through train test split. Test proportion:\")\n","    print(testproportion)\n","    return(dtrain,dvalid,test)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vsT0IecPhZk5"},"source":["# Build Pipeline <a name='buildpipe' />\n","\n","Create pipeline objects to perform final data preparation steps for training and inference.\n","\n","Note that cleanup on the training dataset is completed upstream in the [data cleanup notebook](https://github.com/ryanmark1867/end_to_end_deep_learning_liveproject/blob/master/notebooks/data_cleanup.ipynb). \n","- The pipelines only accomplish the subset of preparation that is required for both training and inference\n","- Because the scoring data coming in for inference is forced by the web deployment to avoid the invalid values that the data cleanup notebook deals with, the pipelines don't have to deal with those problems.\n","\n","<a href=#linkanchor>Back to link list</a>"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NWcj0WkChZk5","executionInfo":{"status":"ok","timestamp":1666403097423,"user_tz":240,"elapsed":16,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"11b19988-0adf-4527-c93d-3bc1e1700789"},"outputs":[{"output_type":"stream","name":"stdout","text":["collist is:  ['neighbourhood_group', 'neighbourhood', 'room_type']\n","textcols is:  []\n","continuouscols is:  ['minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count']\n","excludefromcolist is:  ['price', 'id', 'latitude', 'longitude', 'host_name', 'last_review', 'name', 'host_name', 'availability_365']\n"]}],"source":["print(\"collist is: \",str(collist))\n","print(\"textcols is: \",str(textcols))\n","print(\"continuouscols is: \",str(continuouscols))\n","print(\"excludefromcolist is: \",str(excludefromcolist))"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TRQFrVUDhZk6","executionInfo":{"status":"ok","timestamp":1666403097759,"user_tz":240,"elapsed":346,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"92effd96-7172-4408-b109-09eb13eb293b"},"outputs":[{"output_type":"stream","name":"stdout","text":["fill empty xform\n","col is  neighbourhood_group\n","col is  neighbourhood\n","col is  room_type\n","transform col is  neighbourhood_group\n","after transform col is  neighbourhood_group\n","transform col is  neighbourhood\n","after transform col is  neighbourhood\n","transform col is  room_type\n","after transform col is  room_type\n","Through train test split. Test proportion:\n","0.02\n","cat col is neighbourhood_group\n","cat col is neighbourhood\n","cat col is room_type\n","cont col is minimum_nights\n","cont col is number_of_reviews\n","cont col is reviews_per_month\n","cont col is calculated_host_listings_count\n","cat col is neighbourhood_group\n","cat col is neighbourhood\n","cat col is room_type\n","cont col is minimum_nights\n","cont col is number_of_reviews\n","cont col is reviews_per_month\n","cont col is calculated_host_listings_count\n","keras variables defined\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: ResourceWarning: unclosed file <_io.BufferedWriter name='/content/drive/MyDrive/machine_learning_tabular_book/code/tabpfn_basics/pipelines/sc_delay_piplelinesep4_2022.pkl'>\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:53: ResourceWarning: unclosed file <_io.BufferedWriter name='/content/drive/MyDrive/machine_learning_tabular_book/code/tabpfn_basics/pipelines/sc_delay_pipleline_keras_prepsep4_2022.pkl'>\n","ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"]}],"source":["# master block to invoke pipeline\n","\n","# build fully qualified names for the files for saving the pipelines\n","pipeline_path = get_pipeline_path()\n","pipeline1_file_name = os.path.join(pipeline_path,'sc_delay_pipleline'+modifier+'.pkl')\n","pipeline2_file_name = os.path.join(pipeline_path,'sc_delay_pipleline_keras_prep'+modifier+'.pkl')\n","\n","# define column lists:\n","# collist,continuouscols,textcols = def_col_lists()\n","\n","# create objects of the pipeline classes\n","fe = fill_empty()\n","ec = encode_categorical()\n","pk = prep_for_keras_input()\n","pk_valid = prep_for_keras_input()\n","pk_test = prep_for_keras_input()\n","\n","# need to implement the pipeline in two parts:\n","# 1. fill empty + encode categoricals\n","# 2. prep for Keras\n","# because part 1 needs to be applied to the entire dataset and part 2 to the individual train, validate, and test sets\n","\n","\n","sc_delay_pipeline = Pipeline([('fill_empty',fe),('encode_categorical',ec)])\n","# need to have distinct pipeline objects for each subset of the dataset: train, validated and test\n","sc_delay_pipeline_keras_prep = Pipeline([('prep_for_keras',pk)])\n","sc_delay_pipeline_keras_prep_valid = Pipeline([('prep_for_keras',pk_valid)])\n","sc_delay_pipeline_keras_prep_test = Pipeline([('prep_for_keras',pk_test)])\n","\n","\n","\n","# provide the value for each parameter of each of the pipeline classes\n","\n","sc_delay_pipeline.set_params(fill_empty__collist = collist, fill_empty__continuouscols = continuouscols,\n","                            fill_empty__textcols = textcols,encode_categorical__col_list = collist)\n","sc_delay_pipeline_keras_prep.set_params(prep_for_keras__collist = collist,\n","                            prep_for_keras__continuouscols = continuouscols,\n","                            prep_for_keras__textcols = textcols)\n","sc_delay_pipeline_keras_prep_valid.set_params(prep_for_keras__collist = collist,\n","                            prep_for_keras__continuouscols = continuouscols,\n","                            prep_for_keras__textcols = textcols)\n","sc_delay_pipeline_keras_prep_test.set_params(prep_for_keras__collist = collist,\n","                            prep_for_keras__continuouscols = continuouscols,\n","                            prep_for_keras__textcols = textcols)\n","\n","# fit the input dataset to the pipeline\n","\n","# first fit the first segment of pipeline on the whole dataset\n","X = sc_delay_pipeline.fit_transform(merged_data)\n","max_dict = ec.max_dict\n","# then split dataset\n","dump(sc_delay_pipeline, open(pipeline1_file_name,'wb'))\n","dump(sc_delay_pipeline_keras_prep, open(pipeline2_file_name,'wb'))\n","dtrain, dvalid, test = get_train_validation_test(X)\n","# then apply second portion of pipeline to each subset\n","# need to have a distinct object for each to prevent first object impacting others\n","\n","X_train_list = sc_delay_pipeline_keras_prep.fit_transform(dtrain)\n","#X_valid_list = sc_delay_pipeline_keras_prep_valid.fit_transform(dvalid)\n","X_test_list = sc_delay_pipeline_keras_prep_test.fit_transform(test)\n","\n","print(\"keras variables defined\")\n","#print(\"X_train_list\",X_train_list)\n","\n"]},{"cell_type":"code","source":["collist"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRWeWbsYQuzX","executionInfo":{"status":"ok","timestamp":1666403097760,"user_tz":240,"elapsed":113,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"c06b310e-4ac3-41b9-ddef-572c372a7807"},"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['neighbourhood_group', 'neighbourhood', 'room_type']"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["# Features are\n","# neighbourhood_group\n","# neighbourhood\n","# room_type\n","# minimum_nights\n","# number_of_reviews\n","# reviews_per_month\n","# calculated_host_listings_count\n","\n","#X_train_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFfTDWwlKf-K","executionInfo":{"status":"ok","timestamp":1666403097761,"user_tz":240,"elapsed":102,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"4f34ba6f-58fb-4af1-e93d-2126bad17033"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([1, 0, 1, 1, 1, 2, 2, 1, 1, 3, 2, 2, 1, 1, 3, 1, 3, 2, 1, 1, 2, 3,\n","        0, 1, 2, 1, 2, 1, 3, 1, 1, 3, 2, 1, 2, 1, 3, 1, 2, 1, 2, 2, 2, 2,\n","        1, 3, 3, 0, 1, 3, 1, 1, 1, 2, 2, 2, 1, 2, 1, 3, 3, 1, 2, 1, 2, 2,\n","        1, 2, 1, 3, 1, 1, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 3,\n","        2, 0, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1,\n","        1, 1, 1, 1, 2, 3, 2, 1, 3, 2, 2, 2, 2, 2, 1, 2, 0, 0, 1, 2, 3, 1,\n","        1, 2, 1, 2, 3, 2, 2, 3, 1, 2, 2, 3, 2, 2, 2, 1, 1, 1, 1, 1, 2, 2,\n","        1, 3, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 3, 1, 2, 1, 1, 1, 0, 1, 1,\n","        2, 2, 1, 3, 3, 2, 2, 1, 1, 2, 1, 1, 1, 3, 1, 2, 1, 2, 2, 1, 1, 2,\n","        2, 1, 2, 0, 2, 2, 3, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 3, 2, 3, 2,\n","        3, 3, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 2, 3, 1, 2, 2, 2, 2, 2,\n","        1, 2, 0, 3, 2, 1, 0, 1, 3, 3, 1, 1, 2, 3, 1, 2, 1, 2, 1, 2, 1, 1,\n","        1, 1, 2, 3, 2, 3, 3, 1, 1, 1, 1, 2, 1, 2, 3, 1, 1, 1, 2, 1, 1, 2,\n","        2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 1, 3, 1, 2, 3, 2, 2, 1, 1, 2, 3,\n","        1, 1, 3, 1, 1, 2, 2, 1, 3, 3, 1, 2, 1, 2, 1, 3, 2, 2, 2, 1, 2, 2,\n","        2, 1, 2, 3, 2, 1, 2, 3, 2, 2, 1, 2, 2, 3, 2, 1, 1, 2, 2, 2, 3, 2,\n","        2, 1, 1, 2, 3, 2, 1, 3, 2, 1, 3, 2, 1, 1, 3, 2, 1, 1, 2, 1, 2, 3,\n","        2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 1,\n","        1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 3, 1, 3, 1, 1, 1, 2, 1,\n","        2, 1, 3, 2, 3, 3, 1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 0, 2, 3,\n","        1, 2, 2, 1, 1, 2, 2, 3, 1, 2, 2, 2, 3, 2, 2, 2, 1, 3, 2, 1, 2, 3,\n","        1, 2, 1, 2, 2, 2, 2, 1, 3, 2, 1, 1, 2, 3, 1, 2, 1, 1, 1, 1, 1, 3,\n","        1, 1, 2, 1, 1, 2, 2, 3, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2,\n","        3, 1, 1, 2, 2, 2, 2, 3, 3, 1, 0, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2,\n","        3, 1, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 3, 2, 2, 2, 1, 2, 2,\n","        2, 2, 2, 3, 3, 2, 1, 2, 1, 2, 4, 1, 3, 2, 2, 2, 3, 1, 2, 2, 2, 1,\n","        1, 3, 4, 1, 2, 3, 3, 2, 1, 2, 1, 1, 1, 3, 3, 3, 2, 2, 3, 1, 2, 0,\n","        1, 1, 2, 3, 1, 1, 0, 1, 3, 1, 2, 3, 2, 3, 2, 1, 0, 2, 1, 1, 1, 1,\n","        1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 0, 2, 2, 3, 2, 2, 2, 2, 1, 1, 3, 1,\n","        1, 2, 2, 1, 1, 1, 1, 2, 0, 2, 2, 1, 1, 1, 2, 2, 2, 1, 3, 2, 2, 2,\n","        2, 2, 1, 2, 1, 2, 2, 3, 2, 2, 3, 2, 2, 1, 2, 1, 1, 2, 2, 2, 1, 1,\n","        3, 2, 2, 3, 2, 1, 1, 2, 1, 2, 2, 2, 2, 3, 1, 2, 1, 1, 1, 1, 1, 2,\n","        1, 2, 2, 1, 2, 3, 3, 2, 3, 2, 0, 1, 1, 2, 3, 2, 2, 1, 1, 2, 2, 2,\n","        1, 2, 3, 1, 1, 1, 3, 2, 3, 1, 1, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 2,\n","        1, 2, 1, 1, 2, 2, 3, 1, 2, 1, 3, 2, 3, 2, 2, 2, 1, 2, 2, 3, 3, 2,\n","        1, 1, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 3, 1, 1, 1, 1, 0, 2,\n","        3, 1, 1, 2, 0, 3, 2, 1, 2, 2, 2, 3, 1, 2, 2, 3, 3, 1, 1, 2, 2, 2,\n","        2, 2, 1, 2, 1, 3, 1, 2, 1, 2, 3, 1, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2,\n","        3, 2, 1, 3, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 1, 1, 3, 1, 1, 3, 2,\n","        1, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, 3, 1, 2, 1, 1, 3, 1, 2, 1, 1, 1,\n","        3, 2, 1, 2, 2, 1, 2, 3, 1, 1, 1, 2, 1, 1, 2, 3, 1, 1, 2, 2, 2, 2,\n","        3, 2, 1, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 0, 1, 3, 1, 1, 1, 1, 1,\n","        2, 1, 2, 2, 1, 2, 2, 1, 1, 3, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 1,\n","        1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1]),\n"," array([ 91, 152, 151,  13, 214,  64,  94,  13,  13, 168, 209,  37,  91,\n","         74,  54,  13,  68,  95,  13,  91,  94, 123,  47, 190, 103,  28,\n","         92, 214, 189,  91,  85,   4, 201,  13,  64,  13, 166,  51,  61,\n","         13,  94,  94, 202,  95,  19, 164,  54, 111, 214, 105,  63,  28,\n","         28, 137, 145, 103,  26,  94,  28,  44,  77,  51, 209,  13, 144,\n","         94, 151, 202, 151,   4,  63,  91, 201, 209,  28, 191, 182,  51,\n","         25,  28,  13,  51,  34,  60, 209,  28,  94, 104, 202, 213, 202,\n","        119,  28,  13,  95,  13, 201, 127,  86, 112, 206,  13,  51, 182,\n","        214, 202,  13,  25,  95, 108,  91, 214,  51, 214,  95,  54, 127,\n","         20, 166, 144, 127, 202,  94,  64, 214,  34, 131, 152,  41,  64,\n","          4, 158,  91,  95,  13, 201, 163,  94, 127,  77,  13, 209, 206,\n","         50, 178, 202,  61,  85, 214, 128,  13,  13,  64, 103,  51, 117,\n","         60, 214, 130,  76,  28, 214, 145, 214,  73,  61, 201,  68,  91,\n","        127,  31,  13,  13,   0,  13,  41,  94,  95,  13, 166,  11,  64,\n","         92, 214, 158,  94, 159,  80,  60,  77,  91,  94, 158,  94,  95,\n","         13, 159, 201, 201,  74,  61, 149, 197,  61, 117,  13,  13,  19,\n","         94,  41, 201,  64,  13, 202, 202, 127, 166,  73, 117,  95,   4,\n","        104,  91,  61,  28,  75,  41,  60,  28,  35, 127, 214,  95, 175,\n","        202,  54,  91, 201, 201,  92,  94, 127,  91,  95,  47, 160,  61,\n","         28,  65, 175,  79, 123,  60, 214, 209, 171,  28, 130, 216,  95,\n","         28,  94,  41,  28, 214, 182, 202, 117, 201,   4, 123,  13, 214,\n","        214,  13, 178, 214, 202, 105,  91,  80, 214, 127,  13,  13, 201,\n","        202,  73,  64,  95,  51, 127, 201,  64,  91, 214,  94,  13, 183,\n","         13, 202, 117,  94,  34,  28, 214,  92, 160,  85,  51, 117,  13,\n","         25,  94, 119,  51,   4, 105,  13,  64, 214, 202,  13,  59, 127,\n","         94, 206,  13, 127,  61, 202,  13,  94,  79, 206,  30, 201, 117,\n","         73,  64,  13, 112,  95, 117,  95,  13,  91,  92, 202,  95, 164,\n","        201, 127,  51,  13,  61, 166, 209,  51,   4,  94,  81,   4,  92,\n","         85,  13, 189, 202,  13,  13, 127,  60, 206, 117,  64, 214, 112,\n","         92,  31,  51,  64,  64,  95,  51,  13,  64,  28,  73, 112,  91,\n","         86,  94,  95,  64, 191, 214, 214, 117, 220, 202,  94, 130,  64,\n","         61,  34, 206,  80, 214,  20, 191,  68,  28,  67, 214, 214,  13,\n","         94, 190, 130,  51,  22,  92, 123,  79,  28, 202,  86,  95, 209,\n","         92,  92, 214, 214,  94,  43, 214,   0, 167,  64,   4,  28, 201,\n","         94, 214,  74,  34,  94,  54,  51, 130, 112,  95, 123, 112, 178,\n","         95,  28,  71,  64,  91, 202, 166,  51, 127,  49, 201,  95, 127,\n","         61, 159, 117, 197,  31, 128, 202, 166, 151,  95,  51,  91,  13,\n","         28,  28,   4,  31,  51,  95,  28,  91,  34,  94,  77,  28, 202,\n","         92, 130,  94,  64,  35, 209,  13, 202, 214,  60, 119,  94, 166,\n","        216,  51, 201,  95,  61, 127, 117,   3,  74, 196,  28,  95, 201,\n","         73,  13, 127, 214,  94,  64, 178,  35, 168,  26,  64,  73, 214,\n","         64,  34,  13, 190,  13,  28,  51, 209, 119,  51,  54,  64, 188,\n","        201, 159, 119,  35,  73,  34,  64,  79,  68, 202,  60, 137,  13,\n","        206,  40, 190, 105,  95, 206, 202, 189, 214, 202,  86,  64,  20,\n","        214,   4,  33,  91, 202, 189,  77,  35,  60, 201,  91,  41,  80,\n","        183,   4,  77, 201,   6, 189, 182, 201,  78, 159,  51, 191,  77,\n","        214,  80, 132,  28,   4,  85, 169,  54,  34,  22,  64, 159, 152,\n","         94,  13,  28,  74,  13, 214,  80, 201,  51,  64,  13,  13,  85,\n","        202, 206, 173, 191, 119, 105, 202, 137,  61, 112,  13, 158,  68,\n","         41,  52, 119,  94,  28,  60, 158,   7, 137, 218,  95,  95,  13,\n","        175, 159, 202, 137, 137,  51, 117,  34,  35,  34, 127,  94,  28,\n","         64, 214, 209, 127, 181,  64, 202, 117,  73, 209,  28, 206, 128,\n","         28,  94, 206,  34,  31,  28,  59,  73,  61, 105,  64,  41, 108,\n","         35, 151,  34, 178, 127,  61,  99,  28,  95,  51,  13,   7,  13,\n","        214, 201,  80,  94,  94,  28, 178,   4,  68,  86, 166,  94, 154,\n","        214,   7, 127,  83,  61,  34,  28,  13, 206, 202, 201, 214,  94,\n","        185,  13,  28,  91, 150, 202, 189,  91, 214, 127, 201, 202,  28,\n","        119,  64, 214, 214, 127, 112,  95, 214,  64,  13,  91,  73,  94,\n","         77,   7, 201, 182, 105,  94,  54,  95, 178,  95, 151,  64,  94,\n","        212,  50,  95,  51,  51,  13,  94,  64,  74,  64, 202,  35, 119,\n","         86, 202, 214, 137,  92, 189, 214,  13,  51,  13, 200, 197, 105,\n","         51,  13,  94, 213, 166, 206, 182,  95, 119, 206, 105, 214, 202,\n","         64, 117, 117,  28, 214,  95,  64, 201, 119,  34,  52, 115,  19,\n","        104, 214,  64, 214, 191, 163, 214, 202, 130,  51,  94,  35,  64,\n","         13, 188,  95,  94, 105, 202, 214, 189, 201,  92, 201,  28,  74,\n","        201,  95,  64, 119,  64,  95,  13,  28, 217,  13,  13, 117,  94,\n","        128, 127,  89, 214, 214,  34,  94,  95,  85, 202,  94, 123,  20,\n","         86,  13,  28,  68,  30, 201,  51, 214,  13,   4,  64,  63, 202,\n","        202,  23,  64, 117, 159, 159, 214,  94,  51,  41,  95, 164, 128,\n","         91, 103, 145,  94,  64,  77,  92,  60,  95,  95,  30, 214,  51,\n","         95, 201, 169, 178,  34, 214,  47,  41, 117,  51,  13, 108,  13,\n","         28,  64,  28,  61, 130,  13, 127,  95,  13,  85,   4,  92,  51,\n","         28,  34,  51,  95,  28,  28, 158, 209,  61,  41, 214,  20, 214,\n","        214, 202,   7,  64,  13, 209, 137,  51,  51]),\n"," array([0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n","        1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1,\n","        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n","        1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n","        0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n","        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 2, 1, 0, 1, 1, 2, 2, 1, 0, 0, 0,\n","        0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n","        1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n","        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n","        0, 0, 0, 2, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n","        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n","        1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 2, 1, 1, 1, 1,\n","        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n","        0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n","        0, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n","        1, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 2, 2, 0, 0, 1,\n","        0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n","        1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 2, 1, 0, 1, 0, 0,\n","        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n","        1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n","        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0, 1, 0,\n","        0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 1,\n","        0, 0, 0, 2, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n","        1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n","        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n","        2, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0,\n","        0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0,\n","        0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n","        0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n","        0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n","        0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n","        2, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0,\n","        0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 0,\n","        0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 0, 0, 0, 1, 0,\n","        0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 2, 0,\n","        0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n","        0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 1, 2, 0, 1, 0, 1, 0, 0,\n","        1, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 2, 0,\n","        1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1,\n","        2, 0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1, 1, 0, 0, 2, 0, 0, 1, 0, 1, 0,\n","        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n","        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n","        0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n","        1, 2, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0]),\n"," array([  1,   2,  30,   2,   1,   4,   1,  30,   1,   3,   3,   4,   7,\n","        180,   3,   2,  30,  30,   2,   4,   2,   1,   4,   4,   1,   6,\n","          3,   1,   5,   4,  50,   1,   2,   1,   5,   3,   2,   3, 120,\n","          2,   3,   2,  30,   1,   1,   1,   7,   2,   1,   3,  30,   3,\n","          2,   2,   2,   3,   3,   2,   1,   3,   1,   4,  30,   4,   2,\n","          2,  14,  10,   3,   1,   3,  10,   3,   3,   2,  30,   2,   4,\n","          2,   2,   2,  30,  45,   1,   2,   1,   1,   3,   5,   2,   7,\n","          1,   2,   1,   1,  30,  30,   1,   7,   3,   2,   4,   2,   3,\n","          2,  30,  30,   3,  31,   4,  14,  30,   3,   1,   1,   1,   3,\n","          1,   1,  30,   1,   1,   3,  20,   2,  21,   1,   3,   5,   7,\n","          4,   3,   2,   3,   2,   1,   2,  60,  30,   1,   2,  21,   7,\n","          2,   2,   1,   2,   1,   2,  35,   2,   5,   2,   7,   1,   1,\n","          3,   2,   2,   1,  12,   1,   7,   4,   3,   1,   2,   4,   5,\n","          2,   1,   3,   7,   2,   4,   2,   1,   2,   2,   5,   1,   3,\n","          5,   2,  30,   4,   5,   4,   1,  30,   7,   1,   4,   3,   1,\n","          2,   3,  10,   3,   2,   5,  90,   5,   2,   2,   2,   7,   7,\n","         14,   4,  30,   1,   3,   1,   1,   5,  10,   2,   1,   1,   1,\n","          2,   1,   3,   1,  30,   5,   1,   1,   2,  28,   4,   1,   2,\n","         30,  14,   5,   2,  30,   1,   5,  28,   3,   3,   2,   2,   5,\n","          2,   1,   1,   3,   2,   7,   2,   1,   1,   3,   2,   2,   1,\n","          2,   1,   2,   2,   4,  60,   3,   2,  25,   1,   5,   4,   1,\n","          2,  30,   5,   4,  10,   1,   2,   1,   1,  30,   3,   2,   5,\n","          4,   2,   2,   4,  10,   2,   5,   6,   5,   1,   3,   2,   1,\n","          4,   7,   1,   1,   1,   3,   7,   5,   1,   1,   7,   4,   2,\n","          5,   2,   3,   2,   7,   1,   2,   2,   2,   3,   2,   1,   2,\n","          1,   5,  30,   1,   2,   1,   1,   3,   1,   3,   3,   2,   4,\n","         29,   2,   3,   4,   3,  31,   6,   1,   3, 999,   1,   5,   2,\n","          1,   3,   3,   1,   2,   2,   1,   3,   3,   1,   3,   1,   7,\n","         30,   3,   4,  30,   2,  10,   2,   3,   1,  29,   2,   3,   1,\n","          2,   7,   7,   3,   3,   2,   1,   1,   3,   1,   3,  30,  30,\n","          1,   2,   3,   7,  30,   4,   7,   1,  30,  30,  31,   1,   3,\n","          1,   1,   3,   7,   5,   2,  30,   2,   3,   1,   3,   4,   5,\n","         15,   4,  20,   1,   3,   3,   1,   2,   1,  30,   3,   3,   5,\n","          5,   3,  30,   1,   4,   4,  30,   2,   1,   1,   3,   7,   7,\n","        120,   3,   3,   1,   7,   3,   1,   2,   5,   5,   1,   7,   4,\n","          3,  30,   1,  30,   3,   5,   3,   3,   5,   5,   1,  30,   1,\n","          4,   1, 480,   2,   1,   5,   7,   1,   5,  29,   2,   2,   4,\n","         30,   2,   1,   5,   3,   4,  30,   1,   4,   1,   1,  30,   5,\n","          4,   3,   2,   1,   3,   2,   2,   1,   1,   1,   6,  31,  30,\n","          2,   2,   1,   3,   1,   1,  13,   1,   1,   2,   3,   4,   1,\n","          2,   2,   5,   2,   1,   2,   2,  25,   1,   1,   2,  29,   4,\n","          3,   4,  29,   1,  28,   2,   3,  30,   5,   7,   1,   2,   2,\n","          3,   1,   3,   3,   2,   5,   3,   1,   3,   1,   7,   3,   2,\n","          2,   1,   2,   1,  30,   2,   1,   6,   9,   1,   2,  10,   2,\n","          2,   1,   2,   1,   3,   8,   2,  30,   7,   1,   1,   4,   2,\n","          2,   1,   1,   3,   2,   2,   1,  30,   2,   2,   2,   5,   1,\n","          3,   6,   1,   1,   1,   6,   5,   7,   3,   1,   2,   2,   3,\n","          3,   3,   1,   3,   2,   1,   5,  30,   3,   3,   1,  14,   1,\n","          2,   3,   2,  30,   3,   2,   1,   5,   3,   2,   4,   2,  30,\n","         60,   1,   1,  15,   3,   3,   3,   1,  30,   1,   6,   1,   2,\n","          2,   2, 300,   1,  30,   2,   1,   1,   1,  10,  30,   2,   1,\n","          7,   1,   3,   3,   1,   2,   2,   2,   2,   5,  11,   2,   5,\n","          2,   3,   1,  90,   1,   4,   1,  14,   2,   1,   7,   2,   3,\n","          2,   6,   5,  21, 300,   5,   2,   7,  30,   5,   4,   1,   4,\n","          1,  30,   4,   2,   7,   5,  30,   7,   1,  30,   2,   1,   2,\n","          7, 365,  30,   1,   5,   3,   2,  30,  30,   1,  30,   3,  30,\n","          3,   2,   1,   2,   1,   3,   3,   4,   2,   1,   5,   1,   1,\n","        100,   1,   2,   7,  30,  14,   2,   3,   1,   1,  30,   3,   2,\n","          1,   1,   2,   3,   2,  30,   2,   4,   2,   2,   2,  12,   1,\n","          1,   1,   1,   6,   3,   3,   5,   1,   1,   3,   4,   3,   5,\n","          2,   1,   1,   2,   4,   2,   3,   3,   1,   2,   1,   3,   1,\n","          5,   1,   4,  14,   2,   4,   5,   3,   2,   2,   1,   4,  30,\n","         30,   4,   4,   2,   2,   2,   4,   2,   2,   4,   3,   2,   2,\n","          2,   3,   1,   5,   3,   3,   4,   1,  20,   5,  30,   3,   1,\n","          3,   2,   1,   4,   3,   4,   1,   2,   4,   2,   1,   1,   2,\n","          4,   6,   2,   6,   2,   4,   3,   4,   1,   3,   1,   7,   6,\n","          1,   2,  30,   4,   3,  30,   2,  30,   2,   1,   1,   1,   1,\n","          2,   5,  30,   2,   4,   3,   3,   1,   2,   6,   1,   1,   2,\n","         30,  30,   1,   1,   2,   6,   4,   1,   2,   1,   2,   1,   1,\n","          1,   3,   7,   5,   5,   3,   3,   1,   3,  30,   5,  20,   4,\n","          2,   1,   1,   2,   3,   1,   1,   3,   2,   3,   2,   3,   2,\n","          3,   3,   2,   2,   2,   2,   1,   1,   1,   1,   2,   3,   2,\n","          1,   2,   2,   2,   2,   2,   4,   1,   5,   3,   2,   5,   1,\n","          2,  20,   2,   3,   2,   2,  30,   1,   5]),\n"," array([ 18,   4,  58,  66,   1,  32, 107,   9,   1,   2,   0,   0,   2,\n","          0,   1,  47,  20,   4,  57, 106,  18,  19,   0,   0,   7,   0,\n","          9,   0,   0,   1,   0,   6,  20,   1,   3,   1,   4,   9,   0,\n","          3,  80,  15,   1,  82, 202,  11,  36, 108,  52,  11, 108,  97,\n","          1,   4,  29,  10,   3,   4, 104,  28,   0,   0,  12,   0,  42,\n","          2,   9,   0,  33,   1,  45,   2,  12,   1,   2,   0,  25,   3,\n","          1,   1,   3,   5,  11,   3,   7,   5,  28,   1,   0,   0,   0,\n","          2,   2,   4,   4,   0,   1, 135,   0,   0,   0,  44,   1,  70,\n","          2,   0,   9,   4,   1,   0,   0,   2,   5,   7,  42,   5,  52,\n","          0,   2,   0,   3,  34,   2,   2,   9,   0,   9,   9,   2,  11,\n","          0,   1, 204,   0,   0,   0,   7,   0,   0,  24,  82,  17,  19,\n","         19,  13,  34,  59,   1,  40,   0, 309,   8,  19,   1,  58,   1,\n","         51,  20,  11,   0,   1,   3,   0,  47,  10,   4,  10,   1,   1,\n","          1,   4, 132,   1,  33, 131, 141,   2,   0,   1,  71,   0,   0,\n","          3,   5,   6,   0,  17,   2,   1,   1,   0,  13,  34,  18,   6,\n","         67,   0,   0,   2,   1,  14,   0,  13,  15,   1,   8,   1,   1,\n","          2,  37,   0,   1,  54,  21,  27,   0,   8,  11,  16,   4,   5,\n","         29,   0,   2,   0,   9,   4,  71,   0,   5,   0,   7, 101,   1,\n","          4,   0,   0,  85,   1,   3,   7,   0,   0,   2,  12,  33,  88,\n","          5,   6,   8,  30, 113,   4,  23,   7,   2,  22, 233,   0,  50,\n","         17,  85,  15,   2,   0,   0,  33,   8,   5,   3,  12,   8,   0,\n","          1,  16,   0,   0,  14,  45,   2, 104,   2,   0,   8,   3,   0,\n","         28,   4,  46,  17,   0,   1,  62,   6,   0,   0,   2, 181, 302,\n","          5,   3,   3,   1,   0,   7,   2,  22,  13,   0,   6,  38,  10,\n","          3,  31,  23,  13,   2,   4,   2,   1, 189,   5,   6,  59,  37,\n","         26,   0,   3,   1,  51,   2,  29,   3,  14,  31,  11,   4,   1,\n","          1,   3,   3,   2,  16,   2,  25,  22,   5,   0,  67,  46,   2,\n","          0,  16,   7, 106,  11,   0,  10,  12,  47,  78,  19,   1,   0,\n","          1,   2,  26,  13,   1,   0,  29,   0,  46,  37,  29, 151,   0,\n","        146,   1,   0,   7,   0,   5,  51,   7,  12, 208,  24,   0,   4,\n","         49,  72,   7,   2,   0,   6,   6,   5,   0,   4,   0,   1,  12,\n","         13,   0,   0,   6,   0,   0,   0, 115,  19,  19,   9,   0,  12,\n","          8,  22,   0,   6, 104,   3,   0,  37, 238,   1,  20,   0,   5,\n","         13,   3,   6,   0,   2,   4,   5,   7, 202,  15,   0,   0,  13,\n","          0,   4,  14, 145,   2,  14,   0,   0,  66,   1,  72,  13,   5,\n","          2,   2,   3,   0,   0,  42,  13,  38,   5,   2,  15,   0,   3,\n","         31,   4,   0,   0,   8,  24,   4,   2,   1,   0,  10,   0,   5,\n","          0,   4,   0,   2,  21,  12,   0,   2,   8,  17,  76,   0,   1,\n","         87,   1,  34,   4,  37,   0,   1,   2,   1,  16,  13, 177,   0,\n","         89, 101,  10,   2,  17,   3,   4,   2,  11,  25,   0,  37,   0,\n","          8,  16,  33,   1,  16,  21,   0,  11,   5,  45,   0,   0,   9,\n","         85,   0,   1,   1,   0,  85,  39,   4,  72,   1,  77,   0,  26,\n","          7, 282,   7,   1,  11, 120,  36,  31,  12,   0,   0,  57,  35,\n","         76, 108,  19,   5,  11,  11,  20,  23,  19,  14,   0,  14,  35,\n","         11, 156,  52,   2,   9,   0,  86,   4,   1,   0,  93,  10,  13,\n","          1,  42,   7,  75,   0,   2,   2,   0,  12,  37,  18,   0,  85,\n","         40,   2,   2,  23,  72,   1,   1,   0,   4,   5,   7,   1,   1,\n","          1, 155,   0,   5,   8,   6,  48,   0,   7,  10,   2,   4,  20,\n","         15,   6,   1,   0,   4,   0,   0,   2,  54,   2,   0,   4,   1,\n","          0,   3,   0,   0,  82,  33,  17,   1,   2,   1,  10,  23,   8,\n","          9,   4,   2,   1,   0,  16,   0,   2,   5,  84,   0,  82,  24,\n","          0,  94,   6,   0,  53,   3,  40,   0,   9,   1,  21,   8,  57,\n","        156,   6,   3,   0,  62,   8,   1,   1,  10, 189,   0,  17,   2,\n","          9, 108,  66,   0,   1,   0,  83,  26,   1, 108,   1,   1,   1,\n","          8,   2,  45,   0,   8,   6,   0,   1,  28,   7,  13,   1,  81,\n","          0,   1,   1,  18,   4,  11,  16,   0,   0,   0,   1,   3,   0,\n","         58,   5,  22, 249,  32,   4,   1,  17,   7,   1,   2,   0,  25,\n","          0, 358,  59,  26,   0,   0,   6,   2,  95,  25,   6,   3,   5,\n","         61, 213,   0,   6,  95,   0,   1,  33,  20,  12,   2,   0,  31,\n","          0,   0,  31,   1,   1, 115, 206,  38,   1,  45,  77,   4,   8,\n","         33,   0,  44,   5,  68,   4,   7,   1,   1,  13,   2,   0,  24,\n","          1,   0,   3,  27,   1,   6,   2,  72,   1,   0,  65,   0,   0,\n","          8,  22,   0,   7,   1, 150,  22,   0, 540,   8,  82,   0,  23,\n","          0,  47,   1,   0, 109,  88,  42,  65,   1,   3,   1,  16,   0,\n","          0,   2,   4,   2,  16,  18,   3,   7,   2,   4,  10, 201,  70,\n","          3,  36, 114,   0, 214, 136,   4, 233,  11,   6,   0,   0,  31,\n","         16,   1,   0,  82,   0,   6,  66,   0, 155, 163,   5,   0,  43,\n","         16,   4,   0,   2,   4,  22,   0,   2, 205,   0,   0,  29,   1,\n","          0,  21,  11,   0,  44,  10,   6,  14,  15,   4,   2,  25,   0,\n","          0,  16,   0,   0,  25,   2,   1,   4,   8,   0,   0,   0,   2,\n","         17,  95,   0,  35,   1,   4,  11,   0,   0,  25,   0,   5, 166,\n","          2,  33,  88,  25,   5,  80,   2,   1, 148,  93,  40,   8,  21,\n","          4,   7,  20,   0,   4,  11, 115,   0,   0,  10,   3,   7,   6,\n","          0,   0,  34,   1,   0,  34,   0,   1,   0]),\n"," array([8.700e-01, 3.900e-01, 8.100e-01, 2.420e+00, 5.000e-02, 5.600e-01,\n","        1.800e+00, 3.400e-01, 1.400e-01, 2.000e+00, 0.000e+00, 0.000e+00,\n","        3.800e-01, 0.000e+00, 3.000e-02, 1.810e+00, 3.400e-01, 2.100e-01,\n","        1.250e+00, 2.820e+00, 2.340e+00, 8.030e+00, 0.000e+00, 0.000e+00,\n","        2.330e+00, 0.000e+00, 4.300e-01, 0.000e+00, 0.000e+00, 2.000e-02,\n","        0.000e+00, 4.100e-01, 1.380e+00, 1.200e-01, 3.000e+00, 1.000e-02,\n","        1.900e-01, 4.100e-01, 0.000e+00, 1.200e-01, 1.370e+00, 1.800e-01,\n","        1.000e-02, 3.380e+00, 4.530e+00, 5.000e+00, 1.640e+00, 1.360e+00,\n","        8.200e-01, 3.300e-01, 3.120e+00, 2.750e+00, 4.000e-02, 1.350e+00,\n","        4.420e+00, 2.400e-01, 2.900e-01, 1.200e-01, 1.650e+00, 2.130e+00,\n","        0.000e+00, 0.000e+00, 2.200e-01, 0.000e+00, 2.450e+00, 2.700e-01,\n","        9.000e-02, 0.000e+00, 1.150e+00, 2.000e-02, 1.820e+00, 4.000e-02,\n","        1.080e+00, 5.100e-01, 3.200e-01, 0.000e+00, 1.040e+00, 3.000e-01,\n","        5.000e-02, 2.000e-02, 6.000e-02, 2.800e-01, 1.800e-01, 4.400e-01,\n","        4.880e+00, 5.200e-01, 1.910e+00, 4.900e-01, 0.000e+00, 0.000e+00,\n","        0.000e+00, 5.000e-02, 9.000e-02, 1.500e-01, 1.400e-01, 0.000e+00,\n","        3.700e-01, 7.670e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.710e+00,\n","        1.000e+00, 2.580e+00, 2.000e+00, 0.000e+00, 3.600e-01, 4.000e+00,\n","        5.000e-01, 0.000e+00, 0.000e+00, 3.200e-01, 1.100e-01, 2.300e-01,\n","        1.270e+00, 1.300e-01, 2.660e+00, 0.000e+00, 6.000e-02, 0.000e+00,\n","        8.000e-02, 2.010e+00, 7.000e-02, 1.300e+00, 2.000e-01, 0.000e+00,\n","        5.300e-01, 2.600e-01, 1.100e-01, 3.900e-01, 0.000e+00, 1.400e-01,\n","        3.510e+00, 0.000e+00, 0.000e+00, 0.000e+00, 2.300e-01, 0.000e+00,\n","        0.000e+00, 1.150e+00, 1.600e+00, 3.600e-01, 1.110e+00, 1.600e+00,\n","        1.600e-01, 7.100e-01, 8.400e-01, 2.000e-02, 1.150e+00, 0.000e+00,\n","        2.860e+00, 2.550e+00, 1.580e+00, 2.000e-02, 2.740e+00, 1.600e-01,\n","        3.710e+00, 3.730e+00, 4.500e-01, 0.000e+00, 1.600e-01, 1.300e-01,\n","        0.000e+00, 1.280e+00, 8.330e+00, 1.110e+00, 3.300e-01, 2.000e-02,\n","        1.600e-01, 4.000e-02, 1.700e-01, 4.130e+00, 5.000e-02, 8.500e-01,\n","        2.920e+00, 3.040e+00, 4.000e-02, 0.000e+00, 1.000e+00, 2.810e+00,\n","        0.000e+00, 0.000e+00, 6.000e-02, 2.100e-01, 2.300e-01, 0.000e+00,\n","        3.200e-01, 1.800e-01, 1.200e-01, 1.000e+00, 0.000e+00, 1.580e+00,\n","        7.900e-01, 9.700e-01, 1.400e-01, 8.800e-01, 0.000e+00, 0.000e+00,\n","        1.500e-01, 2.800e-01, 3.040e+00, 0.000e+00, 9.800e-01, 1.600e-01,\n","        2.000e-02, 1.760e+00, 7.000e-02, 8.000e-02, 5.000e-02, 4.700e-01,\n","        0.000e+00, 3.000e-02, 7.700e-01, 3.600e-01, 6.690e+00, 0.000e+00,\n","        1.700e-01, 6.600e-01, 1.370e+00, 4.000e+00, 1.000e-01, 4.730e+00,\n","        0.000e+00, 1.100e-01, 0.000e+00, 2.900e-01, 1.700e-01, 8.990e+00,\n","        0.000e+00, 2.420e+00, 0.000e+00, 1.600e-01, 2.360e+00, 5.900e-01,\n","        1.000e-01, 0.000e+00, 0.000e+00, 3.280e+00, 1.100e-01, 6.000e-02,\n","        1.090e+00, 0.000e+00, 0.000e+00, 7.300e-01, 3.500e-01, 3.600e+00,\n","        2.230e+00, 3.600e-01, 1.800e-01, 2.600e-01, 1.280e+00, 3.180e+00,\n","        2.550e+00, 3.170e+00, 1.400e-01, 2.000e+00, 2.890e+00, 3.010e+00,\n","        0.000e+00, 4.210e+00, 4.050e+00, 1.150e+00, 2.900e-01, 2.600e-01,\n","        0.000e+00, 0.000e+00, 3.460e+00, 5.600e-01, 1.500e-01, 1.300e-01,\n","        9.900e-01, 3.200e-01, 0.000e+00, 2.000e-02, 1.180e+00, 0.000e+00,\n","        0.000e+00, 5.200e-01, 1.330e+00, 2.000e+00, 2.160e+00, 4.000e-01,\n","        0.000e+00, 3.400e-01, 3.500e-01, 0.000e+00, 3.700e-01, 2.350e+00,\n","        8.000e-01, 2.730e+00, 0.000e+00, 1.000e+00, 1.200e+00, 1.300e-01,\n","        0.000e+00, 0.000e+00, 4.000e-02, 4.730e+00, 1.681e+01, 3.900e-01,\n","        4.000e-02, 1.700e+00, 1.000e+00, 0.000e+00, 6.800e-01, 1.300e-01,\n","        4.600e-01, 2.280e+00, 0.000e+00, 4.100e-01, 9.300e-01, 2.900e-01,\n","        1.300e-01, 5.140e+00, 4.100e-01, 1.260e+00, 3.300e-01, 1.450e+00,\n","        9.700e-01, 9.000e-02, 2.630e+00, 2.080e+00, 2.500e-01, 4.180e+00,\n","        4.190e+00, 4.700e-01, 0.000e+00, 4.200e-01, 4.800e-01, 6.100e-01,\n","        6.000e-02, 2.370e+00, 7.400e-01, 4.300e-01, 5.600e-01, 2.750e+00,\n","        8.000e-02, 1.100e-01, 4.500e-01, 1.300e-01, 1.400e-01, 5.000e-02,\n","        1.110e+00, 9.000e-01, 1.160e+00, 1.031e+01, 4.100e-01, 0.000e+00,\n","        3.310e+00, 2.630e+00, 1.600e-01, 0.000e+00, 6.400e-01, 2.440e+00,\n","        5.260e+00, 6.110e+00, 0.000e+00, 2.500e-01, 3.700e-01, 1.410e+00,\n","        2.700e+00, 5.500e-01, 3.000e-02, 0.000e+00, 9.000e-02, 9.000e-02,\n","        1.530e+00, 2.000e-01, 5.000e-02, 0.000e+00, 1.580e+00, 0.000e+00,\n","        2.730e+00, 5.700e-01, 1.010e+00, 1.860e+00, 0.000e+00, 3.130e+00,\n","        4.000e-02, 0.000e+00, 1.790e+00, 0.000e+00, 4.800e-01, 1.830e+00,\n","        4.200e-01, 2.600e-01, 5.890e+00, 3.100e-01, 0.000e+00, 6.000e-02,\n","        9.300e-01, 2.540e+00, 1.980e+00, 2.300e-01, 0.000e+00, 6.900e-01,\n","        1.500e-01, 2.300e-01, 0.000e+00, 1.700e-01, 0.000e+00, 2.000e-02,\n","        3.200e-01, 8.100e-01, 0.000e+00, 0.000e+00, 4.500e-01, 0.000e+00,\n","        0.000e+00, 0.000e+00, 2.990e+00, 6.800e-01, 5.400e-01, 2.700e-01,\n","        0.000e+00, 2.400e-01, 3.200e-01, 2.440e+00, 0.000e+00, 2.300e-01,\n","        1.350e+00, 1.000e-01, 0.000e+00, 4.040e+00, 3.460e+00, 4.000e-02,\n","        3.700e+00, 0.000e+00, 1.700e-01, 2.100e-01, 4.500e-01, 7.000e-02,\n","        0.000e+00, 4.000e-02, 2.100e-01, 3.330e+00, 3.600e-01, 4.350e+00,\n","        3.100e-01, 0.000e+00, 0.000e+00, 4.100e-01, 0.000e+00, 1.600e-01,\n","        4.600e-01, 3.480e+00, 4.000e-02, 1.420e+00, 0.000e+00, 0.000e+00,\n","        1.230e+00, 3.000e-02, 4.270e+00, 1.260e+00, 2.730e+00, 4.000e-02,\n","        1.000e-01, 4.500e-01, 0.000e+00, 0.000e+00, 1.490e+00, 4.290e+00,\n","        1.930e+00, 1.200e-01, 2.600e-01, 6.200e-01, 0.000e+00, 3.000e+00,\n","        1.630e+00, 2.100e-01, 0.000e+00, 0.000e+00, 1.600e-01, 2.020e+00,\n","        1.000e-01, 2.000e+00, 2.000e-02, 0.000e+00, 1.570e+00, 0.000e+00,\n","        1.100e-01, 0.000e+00, 7.100e-01, 0.000e+00, 6.000e-02, 1.060e+00,\n","        3.300e-01, 0.000e+00, 1.300e-01, 1.800e-01, 3.100e-01, 1.460e+00,\n","        0.000e+00, 3.800e-01, 1.020e+00, 1.000e+00, 9.800e-01, 1.000e-01,\n","        5.800e-01, 0.000e+00, 4.000e-02, 2.000e-01, 1.000e+00, 4.600e-01,\n","        2.700e-01, 2.490e+00, 0.000e+00, 2.780e+00, 3.400e+00, 7.600e-01,\n","        1.250e+00, 5.930e+00, 7.000e-02, 6.400e-01, 1.500e-01, 2.200e-01,\n","        4.700e-01, 0.000e+00, 9.200e-01, 0.000e+00, 1.850e+00, 4.400e-01,\n","        4.900e-01, 1.000e+00, 2.000e+00, 7.700e-01, 0.000e+00, 3.700e-01,\n","        5.000e+00, 9.700e-01, 0.000e+00, 0.000e+00, 8.900e-01, 2.870e+00,\n","        0.000e+00, 1.000e+00, 4.000e-02, 0.000e+00, 4.650e+00, 1.030e+00,\n","        2.700e-01, 2.930e+00, 3.000e-02, 3.660e+00, 0.000e+00, 7.220e+00,\n","        1.390e+00, 5.510e+00, 2.600e-01, 4.000e-02, 1.330e+00, 3.910e+00,\n","        3.940e+00, 9.000e-01, 3.700e-01, 0.000e+00, 0.000e+00, 3.070e+00,\n","        2.470e+00, 1.290e+00, 2.780e+00, 9.900e-01, 1.600e-01, 1.300e-01,\n","        6.400e-01, 4.050e+00, 7.600e-01, 6.600e-01, 2.020e+00, 0.000e+00,\n","        4.900e-01, 7.800e-01, 3.100e-01, 3.590e+00, 1.950e+00, 5.000e-02,\n","        2.100e-01, 0.000e+00, 3.830e+00, 6.400e-01, 3.600e-01, 0.000e+00,\n","        4.730e+00, 1.200e-01, 3.500e-01, 1.000e+00, 1.210e+00, 2.960e+00,\n","        1.440e+00, 0.000e+00, 6.000e-02, 9.000e-02, 0.000e+00, 2.980e+00,\n","        2.000e+00, 4.200e-01, 0.000e+00, 3.520e+00, 2.300e+00, 4.000e-02,\n","        4.000e-02, 3.710e+00, 1.300e+00, 1.000e+00, 1.000e+00, 0.000e+00,\n","        1.800e-01, 5.000e+00, 1.900e-01, 2.000e-02, 1.000e+00, 1.200e-01,\n","        2.390e+00, 0.000e+00, 5.900e-01, 6.900e-01, 2.800e-01, 3.700e+00,\n","        0.000e+00, 3.100e-01, 1.500e-01, 2.000e+00, 1.300e-01, 1.020e+00,\n","        2.600e-01, 3.100e-01, 1.100e-01, 0.000e+00, 9.000e-02, 0.000e+00,\n","        0.000e+00, 6.000e-01, 1.610e+00, 4.000e-02, 0.000e+00, 9.800e-01,\n","        1.000e+00, 0.000e+00, 2.100e-01, 0.000e+00, 0.000e+00, 1.580e+00,\n","        2.610e+00, 4.700e-01, 5.000e-02, 7.000e-02, 1.000e+00, 2.300e-01,\n","        6.600e-01, 3.800e-01, 2.870e+00, 4.900e-01, 1.000e-01, 3.000e-02,\n","        0.000e+00, 4.800e-01, 0.000e+00, 3.200e-01, 1.200e-01, 2.800e+00,\n","        0.000e+00, 1.430e+00, 6.610e+00, 0.000e+00, 1.220e+00, 4.500e-01,\n","        0.000e+00, 2.360e+00, 8.000e-02, 1.540e+00, 0.000e+00, 2.430e+00,\n","        2.000e-02, 4.000e-01, 2.290e+00, 1.570e+00, 4.210e+00, 1.890e+00,\n","        2.000e-01, 0.000e+00, 3.330e+00, 2.200e-01, 4.100e-01, 2.000e-02,\n","        1.600e+00, 3.900e+00, 0.000e+00, 8.000e-01, 7.900e-01, 7.400e-01,\n","        2.380e+00, 2.370e+00, 0.000e+00, 4.000e-02, 0.000e+00, 4.470e+00,\n","        9.100e-01, 4.000e-02, 2.140e+00, 1.000e-02, 2.000e-02, 1.500e-01,\n","        4.210e+00, 9.000e-02, 1.010e+00, 0.000e+00, 1.000e-01, 1.000e-01,\n","        0.000e+00, 1.000e+00, 8.940e+00, 1.300e-01, 9.900e-01, 2.000e-02,\n","        3.170e+00, 0.000e+00, 1.400e-01, 3.000e-02, 6.100e-01, 3.330e+00,\n","        4.780e+00, 1.890e+00, 0.000e+00, 0.000e+00, 0.000e+00, 5.000e-02,\n","        5.000e-02, 0.000e+00, 4.690e+00, 2.880e+00, 4.490e+00, 4.840e+00,\n","        2.640e+00, 8.000e-02, 8.000e-02, 3.700e-01, 5.400e-01, 5.000e-02,\n","        9.000e-02, 0.000e+00, 4.780e+00, 0.000e+00, 5.960e+00, 5.460e+00,\n","        7.500e-01, 0.000e+00, 0.000e+00, 3.500e-01, 9.800e-01, 3.280e+00,\n","        8.800e-01, 1.500e-01, 9.000e-02, 5.500e-01, 5.200e+00, 6.550e+00,\n","        0.000e+00, 1.700e-01, 4.190e+00, 0.000e+00, 1.000e+00, 8.800e-01,\n","        4.800e-01, 1.140e+00, 5.000e-02, 0.000e+00, 5.700e-01, 0.000e+00,\n","        0.000e+00, 2.740e+00, 2.000e-02, 3.000e-02, 2.610e+00, 2.890e+00,\n","        1.300e+00, 5.000e-02, 3.530e+00, 9.900e-01, 3.100e-01, 4.300e-01,\n","        8.200e-01, 0.000e+00, 1.240e+00, 1.170e+00, 1.450e+00, 2.550e+00,\n","        2.600e-01, 1.600e-01, 2.000e-02, 1.720e+00, 1.900e-01, 0.000e+00,\n","        2.990e+00, 1.000e+00, 0.000e+00, 2.100e-01, 5.500e-01, 7.900e-01,\n","        5.900e-01, 5.000e-02, 6.990e+00, 3.000e-02, 0.000e+00, 1.000e+01,\n","        0.000e+00, 0.000e+00, 1.600e-01, 1.040e+00, 0.000e+00, 3.800e-01,\n","        1.000e+00, 2.420e+00, 2.300e-01, 0.000e+00, 6.950e+00, 1.200e-01,\n","        1.830e+00, 0.000e+00, 7.800e-01, 0.000e+00, 1.140e+00, 5.000e-02,\n","        0.000e+00, 2.370e+00, 2.560e+00, 1.110e+00, 1.800e+00, 5.000e-02,\n","        1.400e-01, 3.000e-01, 3.700e-01, 0.000e+00, 0.000e+00, 6.000e-02,\n","        2.350e+00, 9.100e-01, 4.700e-01, 1.380e+00, 2.800e-01, 5.680e+00,\n","        2.800e-01, 5.500e-01, 4.200e-01, 4.370e+00, 4.790e+00, 1.900e-01,\n","        8.400e-01, 2.470e+00, 0.000e+00, 2.530e+00, 3.590e+00, 7.000e-02,\n","        3.550e+00, 4.520e+00, 7.300e-01, 0.000e+00, 0.000e+00, 8.400e-01,\n","        4.600e-01, 1.500e-01, 0.000e+00, 1.130e+00, 0.000e+00, 5.000e-01,\n","        2.240e+00, 0.000e+00, 2.110e+00, 3.550e+00, 9.000e-02, 0.000e+00,\n","        9.900e-01, 3.100e-01, 5.000e-02, 0.000e+00, 4.000e-02, 2.790e+00,\n","        2.550e+00, 0.000e+00, 4.000e-02, 3.260e+00, 0.000e+00, 0.000e+00,\n","        1.170e+00, 1.100e-01, 0.000e+00, 2.600e-01, 1.260e+00, 0.000e+00,\n","        1.110e+00, 2.100e-01, 1.600e-01, 3.800e-01, 8.400e-01, 8.000e-02,\n","        1.360e+00, 1.300e+00, 0.000e+00, 0.000e+00, 4.900e-01, 0.000e+00,\n","        0.000e+00, 9.000e-01, 2.000e+00, 5.000e-02, 3.900e-01, 7.400e-01,\n","        0.000e+00, 0.000e+00, 0.000e+00, 2.000e+00, 1.250e+00, 2.540e+00,\n","        0.000e+00, 1.620e+00, 1.000e+00, 9.000e-02, 4.200e-01, 0.000e+00,\n","        0.000e+00, 4.550e+00, 0.000e+00, 2.300e-01, 3.760e+00, 1.000e-01,\n","        9.000e-01, 3.020e+00, 2.060e+00, 1.300e-01, 3.930e+00, 1.000e-01,\n","        1.000e+00, 1.168e+01, 2.480e+00, 1.460e+00, 1.600e-01, 5.800e-01,\n","        1.880e+00, 8.200e-01, 1.580e+00, 0.000e+00, 1.800e-01, 4.230e+00,\n","        4.180e+00, 0.000e+00, 0.000e+00, 8.700e-01, 1.500e-01, 7.800e-01,\n","        6.000e+00, 0.000e+00, 0.000e+00, 3.030e+00, 6.000e-02, 0.000e+00,\n","        4.300e-01, 0.000e+00, 5.200e-01, 0.000e+00]),\n"," array([  1,   2,   1,   1,   1,   1,   1,   3,   1,   1,   1,   3,   1,\n","          1,   1,   1,   2,  91,   3,   1,   2,   4,   2,   1,   9,   3,\n","          1,   2,   1,   1,   1,   1,   1,   2,   1,   2,   1,   1,   1,\n","          1,   1,   2,  52,   5,   4,   7,   1,   2,   1,   1,   2,   1,\n","          1, 327,   2,   2,   1,   4,   1,   1,   1,   1,  39,   1,   1,\n","          1,   2,   1,   2,   1,   8,   1,   1,   2,   1,   4,   2,   1,\n","          1,   1,   1,   4,   1,   3,   1,   2,   1,   1,   1,   3,   1,\n","          1,   1,   3,   1,  37,  13,   3,   1,   1,   1,   1,   1,   1,\n","          1,   9,   4,   1,   3,   2,   1,   6,   1,   2,   5,   1,   1,\n","         10,   1,  31,   2,   4,   1,   1,   1,   1,   1,   2,   1,   1,\n","          1,   2,   1,   1,   1,   1,   1,   1,  65,   1,   1,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,   2,   2,   2,   1,   1,   1,\n","          1,   1,   1,   1,   2,   3,   1,   2,   1,   1,   1,   1,   1,\n","          2,   1,   2,   2,   5,   1,   2,   1,   1,   1,   2,   1,   1,\n","          1,   2,   3,   1,   1,   3,   1,  15,   1,   3,   2,   1,   1,\n","          3,   2,   3,   1,   1,   1,   1,   1,   1,   1,   2,   1,   1,\n","          1,   1,   1,   1,   5,   4,   3,   1,   1,   2,   1,   1,   1,\n","          1,   1,   2,   3,  39,   1,   2,   1,   1,   1,   1,   1,   1,\n","         12,   1,   1,   2,   1,   3,   7,   1,   2,   2,   2,   1,   1,\n","          1,  13,   1,   3,   3,   9,   1,   1,   3,   1,   1,   2,   7,\n","          2,   1,   2,   1,   1,   1,   1,   1,  10,   1,   3,   3,   1,\n","          1,   1,   2,   1,   1,   4,   1,   1,   1,  96,   1,   1,   1,\n","          1, 327,   1,   1,   1,   1,   4,   1,   1,   5,   1,   1,   3,\n","          1,   1,   2,   3,  20,   2,   1,   1,   3,   1,   2,   1,   3,\n","          1,   2,   1,   1,   2,   3,   5,   1,   1,   2,   1,   1,   1,\n","          1,   1,   3,   2,   1,   2,   3,   1,   2,   1,   5,   1,   2,\n","         96,   1,   1,   1,   3,   1,   1,   3,   1,   1,   4,   1,   2,\n","          1,   1,   1,   5,   1,   1,   1,   3,   2,   1,   4,   1,   1,\n","          2,   1,   1,   8,   1,   1,   1,   1,   1,   1,   3,   1,   1,\n","          1,   1,   1,   1,   2,   1,   1,   1,   4,   5,   2, 232,  52,\n","          1,   3,   1,   2, 232,   1,   2,   1,  37,   3,   2,   1,   1,\n","         10,   3,   1,   1,   1,  10, 232,   6,   1,   1,   1,   1,   1,\n","          1,   1,   1,   1,   1,   1,   3,   5,   3,   1,   1,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,\n","          1,   1,   2,  12,   1,   1,   1,   2,   1,   1,   2,   1,   1,\n","          1,   6,   4,   4,   1,   1,   1,   2,   3,   1,   1,  65,   1,\n","          1,   1,   1,   1,   1,   1,   1,   1,   1, 327,   4,   1,   2,\n","          4,   2,   1,   1,   1,   1,  14,   1,   1,   2,   8,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   4,  23,\n","          1,   4,   2,   3,   5,   1,   1,   5,   2,   3,   1,   1,   1,\n","        327,   1,   1,   3,   2,   2,   1,   5,   1,  10,   1,  96,   1,\n","          1,   1,   2,   1,   4,   2,   4,   6,   2,   3,   1,   1,   1,\n","          1,   1,   2,   1,   1,   1,   1,   2,   2,   1,   9,   2,   1,\n","          1,   1,   2,   2,   2,   2,   1,   1,   1,   1,   1,   1,   1,\n","          1,   4,   1,   1,   1,   2,   1,   2,   9,   1,   1,   1,   1,\n","          6,   2,   1,   1,   1,   1,   1,   2,   3,   1,   1,   1,   1,\n","          2,   1,   1,   1,   1,   1,   2,   1,   1,   3,   1,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,  65,   1,   1,   3,   2,  17,\n","          1,   1,   1, 232,   1,   4,   1,   1,   2,   1,   1,   1, 103,\n","          1,   1,   1,   1,   3,   3,   1,   1,  96,   1,   1,   2,   1,\n","          1,   1,   1,   1,  65,   1,   1,   1,   5,   2,  91,   1,   1,\n","          1,   1,   1,   1,   1,   1,   2,   1, 327,   1,   1,   2,   3,\n","          1,   2,   1,   1,   2,   2,   1,   1,   1,   2,   1,   1,   1,\n","          2,   1,   2,   1,   1,   1,   1,   2,  96,   5,   1,   1,   1,\n","          2,  87,   1,   1,   1,   1,   1,   1,   4,  39,   1,   1,   1,\n","          1,   4,  49,   2,   1,   3,   1,   1,   1,   1, 121,   1, 103,\n","          3,   1,   4,   2,   4,   1,   1,   2,   1,   2,   1,   1,   1,\n","         28,   1,   2,   1,   1,   1,   3,   1,   1,   1,  52,   1,   1,\n","          4,   1,   1,   1,   1,   2,   2,   1,   1,   2,   1,   1,   1,\n","          1,   8,   1,   1,   1,   1,   1,   2,   1,   3,   1,   3,   1,\n","          3,   1,   3, 327,   1,   2,   1,   1,   1,   3,   2,   1,   4,\n","          1,   1,   1,   4,   1,   1,   1,   3,   1,   1,   8,   1,  33,\n","          1,   5,   1,   1,   3,   3,   1,   1,   1,   1,   3,   1,   1,\n","          1,   1,   1,   1,   1,   1,   1,   2,   1,   1,   6,   1,   1,\n","          1,   1,   6,   1,   2,   1,   2,   1,   1,   1,   1,   1,   1,\n","          1,   2,   2,   1,   3,   1,   2,   1,   3,   2,   1,   1,   3,\n","          1,   1,   1,   2,   1,   1,   4,  65,   1,   1,   1,   2,   2,\n","          1,   1,  14,   1,   1,   1,   2,   1,   1,   1,   1,   4,   1,\n","         33,   1,   1,   5,   2,   1,   1,   1,   1,   1,   1,   2,   1,\n","          1,   1,   1,   1,   1,   1,   1,   3,   3,  20,   2,   1,   1,\n","          1,   2,   1,   2,   1,   1,   3,   1,   1,   1,   1,   3,   1,\n","          2,   2,   3,   1,   1,   1,   3,   1,   6,   8,   1,   1,   2,\n","          2,   1,   1,   1,   1,   1,   1,   1,   1,   2,   1,   8,   4,\n","          1,   1,   1,   2,   1,   1,  49,   1,   1])]"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["max(X_train_list[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XVrn6p37K7bl","executionInfo":{"status":"ok","timestamp":1666403097762,"user_tz":240,"elapsed":93,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"71871046-a127-4ff0-a80e-398dee85c089"},"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["max(X_train_list[1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hfM3wlFbLA1b","executionInfo":{"status":"ok","timestamp":1666403097763,"user_tz":240,"elapsed":49,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"b6e2bc2c-8c09-4eee-bf1b-9b32fd289dde"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["220"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["max(X_train_list[2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1xqH9cIPLpIh","executionInfo":{"status":"ok","timestamp":1666403097764,"user_tz":240,"elapsed":45,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"beaf4ee7-05b2-4688-fd3e-6ae9af4f645d"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["max(X_train_list[3])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MrKXbJlrLxHd","executionInfo":{"status":"ok","timestamp":1666403097765,"user_tz":240,"elapsed":42,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"52ef433e-7f21-4054-f6a9-97cfed71b522"},"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["999"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LGuN1HDIhZk7","executionInfo":{"status":"ok","timestamp":1666403097767,"user_tz":240,"elapsed":39,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"44b8d356-be15-4ab4-8949-3b46d2cacf88"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["958"]},"metadata":{},"execution_count":62}],"source":["len(X_train_list[0])"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A_noFUZNhZk7","executionInfo":{"status":"ok","timestamp":1666403097768,"user_tz":240,"elapsed":36,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"887b7757-4e89-47a8-8c14-b8165b162511"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["958"]},"metadata":{},"execution_count":63}],"source":["len(dtrain.target)"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"FpYlf9UThZk8","executionInfo":{"status":"ok","timestamp":1666403097769,"user_tz":240,"elapsed":33,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"outputs":[],"source":["#len(X_valid_list[0])"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62RPYpY2hZk8","executionInfo":{"status":"ok","timestamp":1666403097771,"user_tz":240,"elapsed":34,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"1aeeb810-0bb0-498e-c696-594f282c09ff"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["46959"]},"metadata":{},"execution_count":65}],"source":["len(dvalid)"]},{"cell_type":"markdown","source":["##Define and fit model <a name='modelfit' />\n","- the deep learning model requires a list of numpy arrays\n","- XGBoost requires a numpy array of lists, so the training and test datasets need to be transformed before the XGBoost model is fit\n","\n","<a href=#linkanchor>Back to link list</a>"],"metadata":{"id":"Y3gWLj4xTURi"}},{"cell_type":"code","source":["# get lists of lists for the training and test datasets\n","list_of_lists_train = []\n","list_of_lists_test = []\n","for i in range(0,7):\n","    list_of_lists_train.append(X_train_list[i].tolist())\n","    list_of_lists_test.append(X_test_list[i].tolist())"],"metadata":{"id":"4im8dysiTfr2","executionInfo":{"status":"ok","timestamp":1666403097773,"user_tz":240,"elapsed":32,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["%%time\n","# convert lists of lists to numpy arrays of lists\n","xgb_X_train = np.array(list_of_lists_train).T\n","xgb_X_test = np.array(list_of_lists_test).T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-PqPdKsHUeKq","executionInfo":{"status":"ok","timestamp":1666403097978,"user_tz":240,"elapsed":236,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"a0d82c78-8a75-4caa-9554-3f427290d9c6"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 1.5 ms, sys: 0 ns, total: 1.5 ms\n","Wall time: 1.51 ms\n"]}]},{"cell_type":"code","source":["xgb_X_train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVhvuamWUjeD","executionInfo":{"status":"ok","timestamp":1666403097979,"user_tz":240,"elapsed":26,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"ab0e80e1-997e-4106-a269-9a1f0c2a83a0"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[  1.  ,  91.  ,   0.  , ...,  18.  ,   0.87,   1.  ],\n","       [  0.  , 152.  ,   2.  , ...,   4.  ,   0.39,   2.  ],\n","       [  1.  , 151.  ,   0.  , ...,  58.  ,   0.81,   1.  ],\n","       ...,\n","       [  2.  , 137.  ,   0.  , ...,   0.  ,   0.  ,  49.  ],\n","       [  1.  ,  51.  ,   1.  , ...,   1.  ,   0.52,   1.  ],\n","       [  1.  ,  51.  ,   0.  , ...,   0.  ,   0.  ,   1.  ]])"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["%%time\n","# train TabPFN model using the same balancing factor as used for the deep learning model: one_weight\n","model_path = get_model_path()\n","#xgb_save_model_path = os.path.join(model_path,'sc_xgbmodel'+modifier+\"_\"+str(experiment_number)+'.txt')\n","#model = XGBClassifier()\n","#model.fit(xgb_X_train, dtrain.target)\n","#model.save_model(xgb_save_model_path)\n","classifier = TabPFNClassifier(device='cuda')\n","start = time.time()\n","classifier.fit(xgb_X_train, dtrain.target)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sx8Koo9dUopH","executionInfo":{"status":"ok","timestamp":1666403098494,"user_tz":240,"elapsed":537,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"7a25f825-32ad-412b-c105-14e9a91b64c4"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading models_diff/prior_diff_real_checkpoint_n_0_epoch_100.cpkt\n","Loading....\n","Using style prior: True\n","MODEL BUILDER <module 'tabpfn.priors.differentiable_prior' from '/usr/local/lib/python3.7/dist-packages/tabpfn/priors/differentiable_prior.py'> <function get_model.<locals>.make_get_batch.<locals>.new_get_batch at 0x7f900fab1e60>\n","Using cuda device\n","init dist\n","Not using distributed\n","DataLoader.__dict__ {'num_steps': 8192, 'get_batch_kwargs': {'batch_size': 1, 'eval_pos_seq_len_sampler': <function train.<locals>.eval_pos_seq_len_sampler at 0x7f8fdab5a3b0>, 'seq_len_maximum': 10, 'device': 'cuda', 'num_features': 100, 'hyperparameters': {'lr': 0.0001, 'dropout': 0.0, 'emsize': 512, 'batch_size': 1, 'nlayers': 12, 'num_features': 100, 'nhead': 4, 'nhid_factor': 2, 'bptt': 10, 'eval_positions': [972], 'seq_len_used': 50, 'sampling': 'mixed', 'epochs': 400, 'num_steps': 8192, 'verbose': False, 'mix_activations': True, 'nan_prob_unknown_reason_reason_prior': 1.0, 'categorical_feature_p': 0.2, 'nan_prob_no_reason': 0.0, 'nan_prob_unknown_reason': 0.0, 'nan_prob_a_reason': 0.0, 'max_num_classes': 10, 'num_classes': 2, 'noise_type': 'Gaussian', 'balanced': False, 'normalize_to_ranking': False, 'set_value_to_nan': 0.1, 'normalize_by_used_features': True, 'num_features_used': <function load_model.<locals>.<lambda> at 0x7f8fdabacb00>, 'num_categorical_features_sampler_a': -1.0, 'differentiable_hyperparameters': {'distribution': 'uniform', 'min': 1000000.0, 'max': 1000001.0}, 'prior_type': 'prior_bag', 'differentiable': True, 'flexible': True, 'aggregate_k_gradients': 8, 'recompute_attn': True, 'bptt_extra_samples': None, 'dynamic_batch_size': False, 'multiclass_loss_type': 'nono', 'output_multiclass_ordered_p': 0.0, 'normalize_with_sqrt': False, 'new_mlp_per_example': True, 'prior_mlp_scale_weights_sqrt': True, 'batch_size_per_gp_sample': None, 'normalize_ignore_label_too': True, 'differentiable_hps_as_style': False, 'max_eval_pos': 1000, 'random_feature_rotation': True, 'rotate_normalized_labels': True, 'canonical_y_encoder': False, 'total_available_time_in_s': None, 'train_mixed_precision': True, 'efficient_eval_masking': True, 'multiclass_type': 'rank', 'done_part_in_training': 0.8425, 'categorical_features_sampler': <function load_model.<locals>.<lambda> at 0x7f8fdabac0e0>, 'num_features_used_in_training': '<function <lambda>.<locals>.<lambda> at 0x7fc575dfb5e0>', 'num_classes_in_training': '<function <lambda>.<locals>.<lambda> at 0x7fc575dfb550>', 'batch_size_in_training': 8, 'bptt_in_training': 1024, 'bptt_extra_samples_in_training': None, 'prior_bag_get_batch': (<function get_model.<locals>.make_get_batch.<locals>.new_get_batch at 0x7f8fdab4a320>, <function get_model.<locals>.make_get_batch.<locals>.new_get_batch at 0x7f900fab14d0>), 'prior_bag_exp_weights_1': 2.0, 'normalize_labels': True, 'check_is_compatible': True}, 'batch_size_per_gp_sample': None, 'get_batch': <function get_model.<locals>.make_get_batch.<locals>.new_get_batch at 0x7f900fab1e60>, 'differentiable_hyperparameters': {'prior_bag_exp_weights_1': {'distribution': 'uniform', 'min': 1000000.0, 'max': 1000001.0}, 'num_layers': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 6, 'min_mean': 1, 'round': True, 'lower_bound': 2}, 'prior_mlp_hidden_dim': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 130, 'min_mean': 5, 'round': True, 'lower_bound': 4}, 'prior_mlp_dropout_prob': {'distribution': 'meta_beta', 'scale': 0.9, 'min': 0.1, 'max': 5.0}, 'noise_std': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 0.3, 'min_mean': 0.0001, 'round': False, 'lower_bound': 0.0}, 'init_std': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 10.0, 'min_mean': 0.01, 'round': False, 'lower_bound': 0.0}, 'num_causes': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 12, 'min_mean': 1, 'round': True, 'lower_bound': 1}, 'is_causal': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'pre_sample_weights': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'y_is_effect': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'prior_mlp_activations': {'distribution': 'meta_choice_mixed', 'choice_values': [<class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Tanh'>, <class 'torch.nn.modules.activation.Tanh'>], 'choice_values_used': [\"<class 'torch.nn.modules.activation.Tanh'>\", \"<class 'torch.nn.modules.linear.Identity'>\", '<function get_diff_causal.<locals>.<lambda> at 0x7fc575dfb670>', \"<class 'torch.nn.modules.activation.ELU'>\"]}, 'block_wise_dropout': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'sort_features': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'in_clique': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'sampling': {'distribution': 'meta_choice', 'choice_values': ['normal', 'mixed']}, 'pre_sample_causes': {'distribution': 'meta_choice', 'choice_values': [True, False]}, 'outputscale': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 10.0, 'min_mean': 1e-05, 'round': False, 'lower_bound': 0}, 'lengthscale': {'distribution': 'meta_trunc_norm_log_scaled', 'max_mean': 10.0, 'min_mean': 1e-05, 'round': False, 'lower_bound': 0}, 'noise': {'distribution': 'meta_choice', 'choice_values': [1e-05, 0.0001, 0.01]}, 'multiclass_type': {'distribution': 'meta_choice', 'choice_values': ['value', 'rank']}}}, 'num_features': 100, 'epoch_count': 0}\n","Style definition of first 3 examples: None\n","Using a Transformer with 25.82 M parameters\n","CPU times: user 490 ms, sys: 102 ms, total: 593 ms\n","Wall time: 649 ms\n"]},{"output_type":"execute_result","data":{"text/plain":["TabPFNClassifier(device='cuda')"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["# get accuracy of TabPFN\n","y_eval, p_eval = classifier.predict(xgb_X_test, return_winning_probability=True)\n","print('Prediction time: ', time.time() - start, 'Accuracy', accuracy_score(test.target, y_eval))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XrbFEVPSFKGN","executionInfo":{"status":"ok","timestamp":1666403100147,"user_tz":240,"elapsed":1663,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"7ba4c772-d192-449b-879b-c604066e39e3"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction time:  1.562025785446167 Accuracy 0.7678936605316974\n"]}]},{"cell_type":"markdown","source":["END OF TABPFN SPECIFIC CODE"],"metadata":{"id":"Cu-frnIMF6s6"}},{"cell_type":"code","source":["\n","# print elapsed time to run the notebook\n","print(\"--- %s seconds ---\" % (time.time() - start_time))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F71e5cmdnLM2","executionInfo":{"status":"ok","timestamp":1666403100150,"user_tz":240,"elapsed":79,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"cd43e938-3edd-4143-de52-b9b587b23833"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["--- 5.9340832233428955 seconds ---\n"]}]},{"cell_type":"code","source":["''' attribution for TabPFN\n","@misc{tabpfn,\n","  doi = {10.48550/ARXIV.2207.01848},\n","  url = {https://arxiv.org/abs/2207.01848},\n","  author = {Hollmann, Noah and Mller, Samuel and Eggensperger, Katharina and Hutter, Frank},\n","  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},\n","  title = {TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second},\n","  publisher = {arXiv},\n","  year = {2022},\n","  copyright = {arXiv.org perpetual, non-exclusive license}\n","}\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"WvUB9gfdA0gh","executionInfo":{"status":"ok","timestamp":1666403100153,"user_tz":240,"elapsed":72,"user":{"displayName":"Mark Ryan","userId":"08045617267833954278"}},"outputId":"57e4a2e4-27e7-41a7-db1c-5b66d113f616"},"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' attribution for TabPFN\\n@misc{tabpfn,\\n  doi = {10.48550/ARXIV.2207.01848},\\n  url = {https://arxiv.org/abs/2207.01848},\\n  author = {Hollmann, Noah and Mller, Samuel and Eggensperger, Katharina and Hutter, Frank},\\n  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},\\n  title = {TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second},\\n  publisher = {arXiv},\\n  year = {2022},\\n  copyright = {arXiv.org perpetual, non-exclusive license}\\n}\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":72}]}],"metadata":{"kernelspec":{"display_name":"try_tf2","language":"python","name":"try_tf2"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}